{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1\n",
    "Ekstrakcja informacji z publikacji naukowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def parse_publication(reference: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Parse academic publication reference and extract structured information.\n",
    "\n",
    "    Expected reference format:\n",
    "    Lastname, I., Lastname2, I2. (Year). Title. Journal, Volume(Issue), StartPage-EndPage.\n",
    "\n",
    "    Example:\n",
    "    Kowalski, J., Nowak, A. (2023). Analiza algorytmów tekstowych. Journal of Computer Science, 45(2), 123-145.\n",
    "\n",
    "    Args:\n",
    "        reference (str): Publication reference string\n",
    "\n",
    "    Returns:\n",
    "        Optional[dict]: A dictionary containing parsed publication data or None if the reference doesn't match expected format\n",
    "    \"\"\"\n",
    "    # TODO: Implement regex patterns to match different parts of the reference\n",
    "    # You need to create patterns for:\n",
    "    # 1. Authors and year pattern\n",
    "    # 2. Title and journal pattern\n",
    "    # 3. Volume, issue, and pages pattern\n",
    "    authors_year_pattern = r\"^(.*?)\\s*\\((\\d{4})\\)\\.\"\n",
    "    title_journal_pattern = r\"\\s*(.*?)\\.\\s*(.*?),\"\n",
    "    volume_issue_pages_pattern = r\"\\s*(\\d+)(?:\\((\\d+)\\))?,\\s*(\\d+)-(\\d+)\\.$\"\n",
    "\n",
    "    # TODO: Combine the patterns\n",
    "    # full_pattern = authors_year_pattern + title_journal_pattern + volume_issue_pages_pattern\n",
    "    full_pattern = authors_year_pattern + title_journal_pattern + volume_issue_pages_pattern\n",
    "\n",
    "    # TODO: Use re.match to try to match the full pattern against the reference\n",
    "    # If there's no match, return None\n",
    "    match = re.match(full_pattern, reference.strip())\n",
    "    if not match: return None\n",
    "\n",
    "    # TODO: Extract information using regex\n",
    "    # Each author should be parsed into a dictionary with 'last_name' and 'initial' keys\n",
    "\n",
    "    # TODO: Create a pattern to match individual authors\n",
    "    author_pattern = r\"^\\s*([^.]+?),\\s*([A-Z])\\.?$\"\n",
    "\n",
    "    # TODO: Use re.finditer to find all authors and add them to authors_list\n",
    "    authors_list = []\n",
    "    authors_str = match.group(1).strip()\n",
    "    for author in re.split(r\"(\\.,)\", authors_str):\n",
    "        author_match = re.match(author_pattern, author)\n",
    "        if author_match:\n",
    "            authors_list.append({'last_name' : author_match.group(1).strip(), 'initial' : author_match.group(2).strip()})\n",
    "\n",
    "    # TODO: Create and return the final result dictionary with all the parsed information\n",
    "    # It should include authors, year, title, journal, volume, issue, and pages\n",
    "\n",
    "    result = {\n",
    "        'authors' : authors_list,\n",
    "        'year' : int(match.group(2)),\n",
    "        'title' : match.group(3).strip(),\n",
    "        'journal' : match.group(4).strip(),\n",
    "        'volume' : int(match.group(5)),\n",
    "        'issue' : int(match.group(6)) if match.group(6) else None,\n",
    "        'pages' : {\n",
    "            'start' : int(match.group(7)),\n",
    "            'end' : int(match.group(8))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "Analiza linków w kodzie HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_links(html: str) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract all links from the given HTML string.\n",
    "\n",
    "    Args:\n",
    "        html (str): HTML content to analyze\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries where each dictionary contains:\n",
    "            - 'url': the href attribute value\n",
    "            - 'title': the title attribute value (or None if not present)\n",
    "            - 'text': the text between <a> and </a> tags\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement a regular expression pattern to extract links from HTML.\n",
    "    # The pattern should capture three groups:\n",
    "    # 1. The URL (href attribute value)\n",
    "    # 2. The title attribute (which might not exist)\n",
    "    # 3. The link text (content between <a> and </a> tags)\n",
    "\n",
    "    pattern = r\"<a\\s*href=\\\"([^\\\"]+)\\\"(?:\\s+title=\\\"([^\\\"]*)\\\")?>(.*?)</a>\"\n",
    "\n",
    "    links = []\n",
    "\n",
    "    # TODO: Use re.finditer to find all matches of the pattern in the HTML\n",
    "    # For each match, extract the necessary information and create a dictionary\n",
    "    # Then append that dictionary to the 'links' list\n",
    "\n",
    "    matches = re.finditer(pattern, html)\n",
    "    for match in matches:\n",
    "        links.append({\n",
    "            'url' : match.group(1),\n",
    "            'title' : match.group(2),\n",
    "            'text' : match.group(3)\n",
    "        })\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3\n",
    "Analiza pliku tekstowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def analyze_text_file(filename: str) -> dict:\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Could not read file: {str(e)}\"}\n",
    "\n",
    "    # Common English stop words to filter out from frequency analysis\n",
    "    stop_words = {\n",
    "        \"the\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"and\",\n",
    "        \"or\",\n",
    "        \"but\",\n",
    "        \"in\",\n",
    "        \"on\",\n",
    "        \"at\",\n",
    "        \"to\",\n",
    "        \"for\",\n",
    "        \"with\",\n",
    "        \"by\",\n",
    "        \"about\",\n",
    "        \"as\",\n",
    "        \"into\",\n",
    "        \"like\",\n",
    "        \"through\",\n",
    "        \"after\",\n",
    "        \"over\",\n",
    "        \"between\",\n",
    "        \"out\",\n",
    "        \"of\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "        \"be\",\n",
    "        \"been\",\n",
    "        \"being\",\n",
    "        \"have\",\n",
    "        \"has\",\n",
    "        \"had\",\n",
    "        \"do\",\n",
    "        \"does\",\n",
    "        \"did\",\n",
    "        \"this\",\n",
    "        \"that\",\n",
    "        \"these\",\n",
    "        \"those\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"from\",\n",
    "        \"there\",\n",
    "        \"their\",\n",
    "    }\n",
    "\n",
    "    # TODO: Implement word extraction using regex\n",
    "    # Find all words in the content (lowercase for consistency)\n",
    "    words = re.findall(r\"\\b[^\\W\\d_]+\\b\", content.lower())\n",
    "    word_count = len(words)\n",
    "\n",
    "    # TODO: Implement sentence splitting using regex\n",
    "    # A sentence typically ends with ., !, or ? followed by a space\n",
    "    # Be careful about abbreviations (e.g., \"Dr.\", \"U.S.A.\")\n",
    "    sentence_pattern = r\"[A-Z](?:.*?)(?<!Prof)(?<!\\.)[.!?](?=\\s+[A-Z]|\\s*$)\"\n",
    "    sentences = re.findall(sentence_pattern, content, re.MULTILINE)\n",
    "    sentence_count = len(sentences)\n",
    "\n",
    "    # TODO: Implement email extraction using regex\n",
    "    # Extract all valid email addresses from the content\n",
    "    email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "    emails = re.findall(email_pattern, content)\n",
    "\n",
    "    # TODO: Calculate word frequencies\n",
    "    # Count occurrences of each word, excluding stop words and short words\n",
    "    # Use the Counter class from collections\n",
    "    words = re.findall(r'\\b(?!' + '|'.join(re.escape(word) + r'\\b' for word in stop_words) + r')[^\\W\\d_]{2,}\\b', content.lower())\n",
    "    frequent_words = dict(Counter(words).most_common(10))\n",
    "\n",
    "    # TODO: Implement date extraction with multiple formats\n",
    "    # Detect dates in various formats: YYYY-MM-DD, DD.MM.YYYY, MM/DD/YYYY, etc.\n",
    "    # Create multiple regex patterns for different date formats\n",
    "    date_patterns = [r\"\\d{4}-\\d{2}-\\d{2}\", r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', r'\\d{1,2}/\\d{1,2}/\\d{4}', r'\\b\\d{2}-\\d{2}-\\d{4}\\b', r'\\b[\\w]+\\s{1}\\d+,\\s\\d+\\b']\n",
    "    dates = []\n",
    "    for pattern in date_patterns:\n",
    "        dates.extend(re.findall(pattern, content))\n",
    "\n",
    "    # TODO: Analyze paragraphs\n",
    "    # Split the content into paragraphs and count words in each\n",
    "    # Paragraphs are typically separated by one or more blank lines\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", content)\n",
    "    words_pattern = r\"\\b[^\\W\\d_]+\\b\"\n",
    "    paragraph_sizes = {}\n",
    "\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        words_in_paragraph = re.findall(words_pattern, paragraph)\n",
    "        paragraph_sizes[i] = len(words_in_paragraph)\n",
    "\n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"emails\": emails,\n",
    "        \"frequent_words\": frequent_words,\n",
    "        \"dates\": dates,\n",
    "        \"paragraph_sizes\": paragraph_sizes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4\n",
    "Implementacja uproszczonego parsera regexpów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class RegEx(ABC):\n",
    "    @abstractmethod\n",
    "    def nullable(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, symbol):\n",
    "        pass\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, RegEx):\n",
    "            return False\n",
    "        return str(self) == str(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "\n",
    "class Empty(RegEx):\n",
    "    def nullable(self):\n",
    "        return False\n",
    "\n",
    "    def derivative(self, symbol):\n",
    "        return Empty()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"∅\"\n",
    "\n",
    "\n",
    "class Epsilon(RegEx):\n",
    "    def nullable(self):\n",
    "        return True\n",
    "\n",
    "    def derivative(self, symbol):\n",
    "        return Empty()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ε\"\n",
    "\n",
    "\n",
    "class Symbol(RegEx):\n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def nullable(self):\n",
    "        return False\n",
    "\n",
    "    def derivative(self, symbol):\n",
    "        if self.symbol == symbol:\n",
    "            return Epsilon()\n",
    "        return Empty()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.symbol\n",
    "\n",
    "\n",
    "class Concatenation(RegEx):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def nullable(self):\n",
    "        return self.left.nullable() and self.right.nullable()\n",
    "\n",
    "    def derivative(self, symbol):\n",
    "        left_derivative = self.left.derivative(symbol)\n",
    "\n",
    "        if isinstance(left_derivative, Empty):\n",
    "            if self.left.nullable():\n",
    "                return self.right.derivative(symbol)\n",
    "            return Empty()\n",
    "\n",
    "        if self.left.nullable():\n",
    "            right_derivative = self.right.derivative(symbol)\n",
    "            if isinstance(right_derivative, Empty):\n",
    "                return Concatenation(left_derivative, self.right)\n",
    "            return Alternative(\n",
    "                Concatenation(left_derivative, self.right), right_derivative\n",
    "            )\n",
    "        else:\n",
    "            return Concatenation(left_derivative, self.right)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.left}{self.right})\"\n",
    "\n",
    "\n",
    "class Alternative(RegEx):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def nullable(self):\n",
    "        return self.left.nullable() or self.right.nullable()\n",
    "\n",
    "    def derivative(self, symbol):\n",
    "        left_derivative = self.left.derivative(symbol)\n",
    "        right_derivative = self.right.derivative(symbol)\n",
    "\n",
    "        if isinstance(left_derivative, Empty):\n",
    "            return right_derivative\n",
    "        if isinstance(right_derivative, Empty):\n",
    "            return left_derivative\n",
    "\n",
    "        return Alternative(left_derivative, right_derivative)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.left}|{self.right})\"\n",
    "\n",
    "\n",
    "class KleeneStar(RegEx):\n",
    "    def __init__(self, expression):\n",
    "        self.expression = expression\n",
    "\n",
    "    def nullable(self):\n",
    "        return True\n",
    "\n",
    "    def derivative(self, symbol):\n",
    "        derivative = self.expression.derivative(symbol)\n",
    "\n",
    "        if isinstance(derivative, Empty):\n",
    "            return Empty()\n",
    "\n",
    "        return Concatenation(derivative, self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.expression})*\"\n",
    "\n",
    "\n",
    "class DFA:\n",
    "    def __init__(self, states, alphabet, transitions, start_state, accept_states):\n",
    "        self.states = states\n",
    "        self.alphabet = alphabet\n",
    "        self.transitions = transitions\n",
    "        self.start_state = start_state\n",
    "        self.accept_states = accept_states\n",
    "\n",
    "    def accepts(self, string):\n",
    "        \"\"\"Check if the DFA accepts the given string.\"\"\"\n",
    "        current_state = self.start_state\n",
    "\n",
    "        for i, symbol in enumerate(string):\n",
    "            if symbol not in self.alphabet:\n",
    "                return False\n",
    "\n",
    "            if (current_state, symbol) not in self.transitions:\n",
    "                return False\n",
    "\n",
    "            current_state = self.transitions[(current_state, symbol)]\n",
    "\n",
    "        return current_state in self.accept_states\n",
    "\n",
    "    def __str__(self):\n",
    "        result = \"DFA:\\n\"\n",
    "        result += f\"  States: {self.states}\\n\"\n",
    "        result += f\"  Alphabet: {self.alphabet}\\n\"\n",
    "        result += f\"  Start State: {self.start_state}\\n\"\n",
    "        result += f\"  Accept States: {self.accept_states}\\n\"\n",
    "        result += \"  Transitions:\\n\"\n",
    "        for (state, symbol), next_state in sorted(self.transitions.items()):\n",
    "            result += f\"    {state} --{symbol}--> {next_state}\\n\"\n",
    "        return result\n",
    "\n",
    "\n",
    "def simplify(regex):\n",
    "    \"\"\"\n",
    "    Simplify regex expressions to canonical form to improve state identification.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        isinstance(regex, Empty)\n",
    "        or isinstance(regex, Epsilon)\n",
    "        or isinstance(regex, Symbol)\n",
    "    ):\n",
    "        return regex\n",
    "\n",
    "    # For alternatives\n",
    "    if isinstance(regex, Alternative):\n",
    "        left = simplify(regex.left)\n",
    "        right = simplify(regex.right)\n",
    "\n",
    "        if isinstance(left, Empty):\n",
    "            return right\n",
    "        if isinstance(right, Empty):\n",
    "            return left\n",
    "\n",
    "        if str(left) == str(right):\n",
    "            return left\n",
    "\n",
    "        return Alternative(left, right)\n",
    "\n",
    "    # For concatenations\n",
    "    if isinstance(regex, Concatenation):\n",
    "        left = simplify(regex.left)\n",
    "        right = simplify(regex.right)\n",
    "\n",
    "        if isinstance(left, Empty) or isinstance(right, Empty):\n",
    "            return Empty()\n",
    "\n",
    "        if isinstance(left, Epsilon):\n",
    "            return right\n",
    "\n",
    "        if isinstance(right, Epsilon):\n",
    "            return left\n",
    "\n",
    "        return Concatenation(left, right)\n",
    "\n",
    "    # For Kleene star\n",
    "    if isinstance(regex, KleeneStar):\n",
    "        inner = simplify(regex.expression)\n",
    "\n",
    "        if isinstance(inner, KleeneStar):\n",
    "            return inner\n",
    "\n",
    "        if isinstance(inner, Epsilon):\n",
    "            return Epsilon()\n",
    "\n",
    "        if isinstance(inner, Empty):\n",
    "            return Epsilon()\n",
    "\n",
    "        return KleeneStar(inner)\n",
    "\n",
    "    return regex\n",
    "\n",
    "\n",
    "def build_dfa(regex: RegEx, alphabet: set[str]) -> Optional[DFA]:\n",
    "    # Initialize data structures\n",
    "    states = set()  # Set of state names (q0, q1, etc.)\n",
    "    state_to_regex = {}  # Maps state names to their regex\n",
    "    accept_states = set()  # Set of accepting state names\n",
    "    transitions = {}  # Maps (state, symbol) pairs to next state\n",
    "    regex_to_state = {}  # Maps string representations of regex to state names\n",
    "\n",
    "    # Initialize state counter for generating unique state names\n",
    "    state_counter = 0\n",
    "        \n",
    "    def new_state():\n",
    "        nonlocal state_counter\n",
    "        state_name = f\"q{state_counter}\"\n",
    "        state_counter += 1\n",
    "        return state_name\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # TODO: Implement the Brzozowski algorithm to convert regex to DFA\n",
    "    # Steps:\n",
    "    # 1. Start with the initial regex as the start state\n",
    "    start_state = new_state()\n",
    "    states.add(start_state)\n",
    "\n",
    "    state_to_regex[start_state] = regex\n",
    "    regex_to_state[regex] = start_state\n",
    "\n",
    "    if regex.nullable():\n",
    "        accept_states.add(start_state)\n",
    "\n",
    "    # 2. For each state and each symbol in the alphabet:\n",
    "    #    - Compute the derivative of the state's regex with respect to the symbol\n",
    "    #    - Simplify the resulting regex\n",
    "    #    - Add a transition from the current state to a state representing this new regex\n",
    "    Q = deque([start_state])\n",
    "\n",
    "    while Q:\n",
    "        current_state = Q.popleft()\n",
    "        current_regex = state_to_regex[current_state]\n",
    "\n",
    "        for symbol in alphabet:\n",
    "            d = current_regex.derivative(symbol)\n",
    "            d = simplify(d)\n",
    "\n",
    "            if d in regex_to_state:\n",
    "                next_state = regex_to_state[d]\n",
    "            else:\n",
    "                next_state = new_state()\n",
    "                states.add(next_state)\n",
    "                state_to_regex[next_state] = d\n",
    "                regex_to_state[d] = next_state\n",
    "                Q.append(next_state)\n",
    "                \n",
    "                if d.nullable():\n",
    "                    accept_states.add(next_state)\n",
    "        \n",
    "            transitions[(current_state, symbol)] = next_state\n",
    "\n",
    "    # 3. States are accepting if their regex is nullable\n",
    "    # 4. Continue until no new states are discovered\n",
    "\n",
    "    # Return the constructed DFA\n",
    "    # You should return DFA(states, alphabet, transitions, start_state, accept_states)\n",
    "    return DFA(states, alphabet, transitions, start_state, accept_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opis do zadania 4\n",
    "Każdy rozważany stan ma unikatową nazwę oznaczoną jako $q_i$, deklarujemy także mapy, które pozwalają nam jednoznacznie dowiedzieć się, która nazwa stanu odnosi się do konkretnego stanu i vice versa. Następnie budujmy DFA zgodnie z algorytmem Brzozowskiego, rozważając pochodne Brzozowskiego dla danego regexa. Sprawdzamy czy w naszych mapach istnieje już zapis nazwy stanu odpowiadającej konkretnemu regexowi: jeśli tak to ten zapis staje się następnym stanem, a jeśli nie to tworzymy nowy zapis, dodajemy elementy do map i aktualizujemy kolejkę. Jeśli dany regex jest nullable to zapisujemy go do zbioru stanów akceptowanych przez DFA. Dla każdego symbolu i każdego stanu zapisujemy odpowiednie przejścia w mapie transitions.\n",
    "\n",
    "Operacje sprawdzania czy regex jest nullable oraz wyznaczania pochodnych Brzozowskiego są oparte na wcześniej przygotowanych strukturach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
